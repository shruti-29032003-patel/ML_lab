{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Numpy**"
      ],
      "metadata": {
        "id": "hSf_5sjMVwb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q72PoFH1-Obq"
      },
      "outputs": [],
      "source": [
        "# Ex-1. Create two numpy arrays of size 4 X 5 and 5 X 4.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.array([[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]])\n",
        "# arr1.T gives the transpose of arr1\n",
        "# Since we want a 5 x 4 matrix, it will be the transpose of a 4 x 5 matrix, i.e. of arr1\n",
        "arr2 = arr1.T\n",
        "\n",
        "print(arr1)\n",
        "print(arr2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-2. Randomly initialize the above arrays\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# The below statements randomly initiliaze arrays of sizes 4x5 and 5x4 respectively\n",
        "# choose the random integers below 100\n",
        "arr1 = np.random.randint(100, size=(4,5));\n",
        "# choose the random integers below 200\n",
        "arr2 = np.random.randint(200, size=(5,4));\n",
        "\n",
        "print(arr1)\n",
        "print(arr2)"
      ],
      "metadata": {
        "id": "f0VUwTWOvuTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-3. Perform matrix multiplication\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.random.randint(10, size=(2, 3))\n",
        "arr2 = np.random.randint(10, size=(3,4))\n",
        "print(arr1)\n",
        "print(arr2)\n",
        "\n",
        "# '@' operator is a shorthand for np.dot() function\n",
        "# when matrices are operands, it performs the matrix multiplication of the matrices\n",
        "arr = arr1 @ arr2\n",
        "print(arr)\n",
        "arrd = np.dot(arr1, arr2)\n",
        "print(arrd)"
      ],
      "metadata": {
        "id": "JIKLLiH3scxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-4. Perform element-wise matrix multiplication\n",
        "\n",
        "# This is also known as 'Hadamard Product' or 'Schur Product'\n",
        "# Individually multiply the elements both the matrices\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.array([[1,2],[3,4]])\n",
        "arr2 = np.array([[4,8],[0,5]])\n",
        "\n",
        "# using np.multiply() method\n",
        "arrm = np.multiply(arr1, arr2)\n",
        "print(arrm)\n",
        "# shorthand for performing np.multiply()\n",
        "arr = arr1 * arr2\n",
        "print(arr)"
      ],
      "metadata": {
        "id": "Rg7N75TYtsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-5. Find mean, median of the first matrix\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr = np.random.randint(100, size=(4,5))\n",
        "print(arr)\n",
        "\n",
        "# calculates mean along the columns\n",
        "hmean = np.mean(arr, axis=0)\n",
        "# calculates mean along the rows\n",
        "vmean = np.mean(arr, axis=1)\n",
        "print(hmean, vmean)\n",
        "\n",
        "# calculates median along the columns\n",
        "hmed = np.median(arr, axis=0)\n",
        "# calculates median along the rows\n",
        "vmed = np.median(arr, axis=1)\n",
        "print(hmed, vmed)"
      ],
      "metadata": {
        "id": "-Oojd_qSu3FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-6. (i) Get the transpose of the matrix that you created.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr = np.random.randint(100, size=(4,5))\n",
        "print(arr)\n",
        "\n",
        "# T property of matrix gives its transpose\n",
        "arr1 = arr.T\n",
        "print(arr1)\n",
        "# Alternatively, we can also use the np.transpose() method\n",
        "arr2 = np.transpose(arr)\n",
        "print(arr2)\n",
        "print()\n",
        "\n",
        "\n",
        "# Ex-6. (ii) Find the determinant of a square matrix.\n",
        "\n",
        "mat = np.random.randint(10, size=(2,2))\n",
        "print(mat)\n",
        "\n",
        "# np.linalg.det() method allows us to find the determinant of a square matrix\n",
        "# round() is used because the det() method may give a minor imprecision while representing floating-point values\n",
        "det1 = np.linalg.det(mat).round()\n",
        "print(det1)"
      ],
      "metadata": {
        "id": "pz_D-LRtv-f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-7. Obtain each row in the second column of the first array.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr = np.random.randint(100, size=(4,5))\n",
        "print(arr)\n",
        "# second column will be the second row in the transpose\n",
        "row = arr.T[1:2]\n",
        "print(row)"
      ],
      "metadata": {
        "id": "PqQmQkB6xEXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-8. Convert Numeric entries(columns) of mtcars.csv to Mean Centered Version.\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "data = []\n",
        "# read the data from the csv file\n",
        "with open(\"mtcars.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for lines in reader:\n",
        "        data.append(lines)\n",
        "\n",
        "data2 = np.array(data)\n",
        "# extract the useful data from the main data\n",
        "# the heading row is not useful\n",
        "data = data[1:]\n",
        "data = np.array(data)\n",
        "# also the car names are not useful\n",
        "data = np.delete(data, 0, axis=1)\n",
        "\n",
        "# convert the data from string to float\n",
        "data = data.astype(float)\n",
        "# calculate column-wise mean\n",
        "mean = np.mean(data, axis=0)\n",
        "for i in range(len(data)):\n",
        "    data[i] = data[i] - mean\n",
        "\n",
        "# modify the original data by replacing the subpart which is modified above\n",
        "data2[1:, 1:] = data\n",
        "\n",
        "# write the data to a new csv file\n",
        "with open(\"mtcars2.csv\", \"w\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(data2)"
      ],
      "metadata": {
        "id": "xK3HfcsR9gXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK**"
      ],
      "metadata": {
        "id": "q_7IyKhFz2-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie Reviews is a set of 2000 movie reviews, 1000 of which are positive and others negative."
      ],
      "metadata": {
        "id": "k-ZuR8Mg5xNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# nltk.download('movie_reviews')\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"punkt\")\n",
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download(\"omw-1.4\")\n",
        "\n",
        "pos_rev = movie_reviews.fileids(\"pos\");\n",
        "neg_rev = movie_reviews.fileids(\"neg\");\n",
        "print(\"The number of positive reviews: \", len(pos_rev), \" and their type is: \", type(pos_rev));\n",
        "print(\"The number of negative reviews: \", len(neg_rev), \" and their type is: \", type(neg_rev));\n",
        "print()\n",
        "\n",
        "# plot a figure for the samples\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "labels = [\"Positive\", \"Negative\"]\n",
        "sizes = [len(pos_rev), len(neg_rev)]\n",
        "colors = [\"#66ff66\", \"#ff6666\"]\n",
        "plt.pie(sizes, labels=labels, colors=colors, startangle=45, shadow=False, autopct=\"%1.2f%%\")\n",
        "plt.axis(\"equal\")\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "# printing with different colors\n",
        "# positive in green\n",
        "# print(\"\\033[92m\", movie_reviews.raw(fileids=pos_rev[random.randint(0, 100)]))\n",
        "# negative in red\n",
        "# print(\"\\033[91m\", movie_reviews.words(neg_rev[random.randint(0, 100)]))\n",
        "\n",
        "positive_review = movie_reviews.raw(fileids=pos_rev[random.randint(0, 100)])\n",
        "positive_review_tokens = word_tokenize(positive_review)\n",
        "print(\"Tokenized positive review: \", positive_review_tokens)\n",
        "print()\n",
        "\n",
        "stopwords_english = stopwords.words(\"english\")\n",
        "print(\"Stop words: \", stopwords_english)\n",
        "print()\n",
        "print(string.punctuation)\n",
        "print()\n",
        "\n",
        "# cleaning the review\n",
        "review_clean = []\n",
        "for word in positive_review_tokens:\n",
        "    if word not in stopwords_english and word not in string.punctuation:\n",
        "        review_clean.append(word)\n",
        "print(\"Cleaned review: \", review_clean)\n",
        "print()\n",
        "\n",
        "# now we'll perform stemming, i.e. converting the review to the root words\n",
        "stemmer = PorterStemmer()\n",
        "review_stem = []\n",
        "for word in review_clean:\n",
        "    review_stem.append(stemmer.stem(word))\n",
        "# but we see that there are too many miniscule imperfections\n",
        "print(\"Stemmed review: \", review_stem)\n",
        "print()\n",
        "\n",
        "# hence we use lematizer to mitigate the amount of imperfections\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "review_lem = []\n",
        "for word in review_clean:\n",
        "    review_lem.append(lemmatizer.lemmatize(word))\n",
        "print(\"Lemmatized review: \", review_lem)"
      ],
      "metadata": {
        "id": "L3lUWdO7z0K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Pandas**"
      ],
      "metadata": {
        "id": "ywQMjkwSw7W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-1. Draw Scatter Plot between SepalLengthCm and SepalWidthCm for “Iris.csv” file with proper labelling.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/iris.csv\")\n",
        "\n",
        "plt.scatter(data[\"sepal.length\"], data[\"sepal.width\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w1--Od8Tw-UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-2. Draw Histogram of SepalLengthCm with proper labelling.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/iris.csv\")\n",
        "\n",
        "# 'bins' is the number of towers in histogram\n",
        "plt.hist(data[\"sepal.length\"], bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qxz97LuI158W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-3. Plot bar chart of Species.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/iris.csv\")\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"variety\"])\n",
        "labels = df[\"variety\"].unique()\n",
        "counts = df[\"variety\"].value_counts().values\n",
        "plt.bar(labels, counts, color=\"#FF6600\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZR5F3dy221gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-4. Count total null values for each column in this dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/iris.csv\")\n",
        "\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "H4d9MeFr6h4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex-5. i) Print first 5 rows of SepalLengthCm. ii) Print from 5th row and onwards and entire column of iris.csv dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/iris.csv\")\n",
        "\n",
        "# first 5 rows of SepalLengthCm\n",
        "print(data.loc[:4, \"sepal.length\"])\n",
        "print()\n",
        "# print from the 5th row onwards and entire column\n",
        "pd.set_option(\"display.max_rows\", 500)\n",
        "print(data.loc[5:, \"sepal.length\"])"
      ],
      "metadata": {
        "id": "Tjf_k5b260CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit Learn**"
      ],
      "metadata": {
        "id": "MLoIUGU9iT8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "target_names = wine.target_names\n",
        "\n",
        "print(\"Features: \", feature_names)\n",
        "print(\"Targets: \", target_names)\n",
        "print()\n",
        "print(X[:5])\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, y, test_size = 0.2, random_state = 1\n",
        ")"
      ],
      "metadata": {
        "id": "xBI5gzrfiXAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}